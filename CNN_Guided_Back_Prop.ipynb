{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAhTLPtl5NDo"
   },
   "source": [
    "# Create Guided Back Propogation Saliency Maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAZlewlJx047"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mne\n",
    "import pywt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.cm as mpl_color_map\n",
    "from torch import nn\n",
    "from scipy import signal\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58BCfJjdHDOc"
   },
   "outputs": [],
   "source": [
    "# EEG Channel Names\n",
    "chan_names = ['Fp1',\n",
    " 'Fp2',\n",
    " 'F7',\n",
    " 'F3',\n",
    " 'Fz',\n",
    " 'F4',\n",
    " 'F8',\n",
    " 'FC5',\n",
    " 'FC1',\n",
    " 'FC2',\n",
    " 'FC6',\n",
    " 'T7',\n",
    " 'C3',\n",
    " 'Cz',\n",
    " 'C4',\n",
    " 'T8',\n",
    " 'CP5',\n",
    " 'CP1',\n",
    " 'CP2',\n",
    " 'CP6',\n",
    " 'AFz',\n",
    " 'P7',\n",
    " 'P3',\n",
    " 'Pz',\n",
    " 'P4',\n",
    " 'P8',\n",
    " 'PO9',\n",
    " 'O1',\n",
    " 'Oz',\n",
    " 'O2',\n",
    " 'PO10',\n",
    " 'AF7',\n",
    " 'AF3',\n",
    " 'AF4',\n",
    " 'AF8',\n",
    " 'F5',\n",
    " 'F1',\n",
    " 'F2',\n",
    " 'F6',\n",
    " 'FT7',\n",
    " 'FC3',\n",
    " 'FC4',\n",
    " 'FT8',\n",
    " 'C5',\n",
    " 'C1',\n",
    " 'C2',\n",
    " 'C6',\n",
    " 'TP7',\n",
    " 'CP3',\n",
    " 'CPz',\n",
    " 'CP4',\n",
    " 'TP8',\n",
    " 'P5',\n",
    " 'P1',\n",
    " 'P2',\n",
    " 'P6',\n",
    " 'PO7',\n",
    " 'PO3',\n",
    " 'POz',\n",
    " 'PO4',\n",
    " 'PO8']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vOyKG6Bpx05A"
   },
   "source": [
    "# 1.) Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLOQ-20Qc7q4"
   },
   "outputs": [],
   "source": [
    "class WLCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    conv2d_by_Leads seperates the batchxleadx32x250 batches into batch*leadx32x250 batches\n",
    "    that can be fed into subConv2d which generates an output for each lead in the batch. the\n",
    "    batch is then cast back into shape batchxleadx1x1 where the output for each lead can be \n",
    "    fed into the linear layer for making inference without combining lead data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(WLCNN,self).__init__()\n",
    "\n",
    "        self.conv1a = nn.Conv2d(61,122, kernel_size=(4,4), stride=(1,3)) \n",
    "        self.a1 = nn.ReLU()\n",
    "        self.conv1a_bn = nn.BatchNorm2d(122)\n",
    "\n",
    "\n",
    "        self.conv1b = nn.Conv2d(122,122, kernel_size=(4,2), stride=(1,3))\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.conv1b_bn = nn.BatchNorm2d(122)\n",
    "        \n",
    "        self.maxPool1 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.conv2a = nn.Conv2d(122,244, kernel_size=(3,4), stride=(2,2))\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.conv2a_bn = nn.BatchNorm2d(244)\n",
    "\n",
    "        self.conv2b = nn.Conv2d(244,244, kernel_size=(4,4), stride=(2,2))\n",
    "        self.a4 = nn.ReLU()\n",
    "        self.conv2b_bn = nn.BatchNorm2d(244)\n",
    "\n",
    "        self.maxPool2=nn.MaxPool2d((2,2))\n",
    "        \n",
    "        \"\"\"\n",
    "        self.conv3a = nn.Conv2d(244,488, kernel_size=(3,2), stride=(1,1))\n",
    "        self.conv3a_bn = nn.BatchNorm2d(488)\n",
    "        self.a5 = nn.ReLU()\n",
    "\n",
    "        self.conv3b = nn.Conv2d(488,488, kernel_size=(2,2), stride=(1,1))\n",
    "        self.a6 = nn.ReLU()\n",
    "        self.conv3b_bn = nn.BatchNorm2d(488)\n",
    "\n",
    "        self.maxPool3=nn.MaxPool2d((2,2))\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fc1 = nn.Linear(244,3) \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    " \n",
    "        #convolve over channels only\n",
    "        x = self.conv1a(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.conv1a_bn(x)\n",
    "\n",
    "      \n",
    "        x = self.conv1b(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.conv1b_bn(x)\n",
    "        \n",
    "        x = self.maxPool1(x)\n",
    "\n",
    "  \n",
    "        x = self.conv2a(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.conv2a_bn(x)\n",
    "\n",
    "\n",
    "        x = self.conv2b(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.conv2b_bn(x)\n",
    "\n",
    "        x = self.maxPool2(x)\n",
    "        \"\"\"\n",
    "        x = self.conv3a(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.conv3a_bn(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        x = self.conv3b(x)\n",
    "        x = self.a6(x)\n",
    "        x = self.conv3b_bn(x)\n",
    "\n",
    "        x = self.maxPool3(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.view(-1,244)\n",
    "\n",
    "        x = self.fc1(x) #a (1x1) output for each lead\n",
    "       # print(x.shape)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "#set to train w/ GPU if available else cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = WLCNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDTS4tPsgu8p"
   },
   "outputs": [],
   "source": [
    "class W3DCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    conv2d_by_Leads seperates the batchxleadx32x250 batches into batch*leadx32x250 batches\n",
    "    that can be fed into subConv2d which generates an output for each lead in the batch. the\n",
    "    batch is then cast back into shape batchxleadx1x1 where the output for each lead can be \n",
    "    fed into the linear layer for making inference without combining lead data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(W3DCNN,self).__init__()\n",
    "\n",
    "        self.conv1a = nn.Conv3d(1,4, kernel_size=(3,4,4), stride=(2,1,3)) \n",
    "        self.a1 = nn.ReLU()\n",
    "        self.conv1a_bn = nn.BatchNorm3d(4)\n",
    "\n",
    "\n",
    "        self.conv1b = nn.Conv3d(4,8, kernel_size=(4,4,2), stride=(2,1,3))\n",
    "        self.a2 = nn.ReLU()\n",
    "        self.conv1b_bn = nn.BatchNorm3d(8)\n",
    "        \n",
    "        self.maxPool1 = nn.MaxPool3d((2,2,2))\n",
    "\n",
    "        self.conv2a = nn.Conv3d(8,16, kernel_size=(3,3,4), stride=(2,2,2))\n",
    "        self.a3 = nn.ReLU()\n",
    "        self.conv2a_bn = nn.BatchNorm3d(16)\n",
    "\n",
    "        self.conv2b = nn.Conv3d(16,32, kernel_size=(2,4,4), stride=(1,2,2))\n",
    "        self.a4 = nn.ReLU()\n",
    "        self.conv2b_bn = nn.BatchNorm3d(32)\n",
    "\n",
    "        self.maxPool2=nn.MaxPool3d((2,2,2))\n",
    "        \n",
    "        \"\"\"\n",
    "        self.conv3a = nn.Conv2d(244,488, kernel_size=(3,2), stride=(1,1))\n",
    "        self.conv3a_bn = nn.BatchNorm2d(488)\n",
    "        self.a5 = nn.ReLU()\n",
    "\n",
    "        self.conv3b = nn.Conv2d(488,488, kernel_size=(2,2), stride=(1,1))\n",
    "        self.a6 = nn.ReLU()\n",
    "        self.conv3b_bn = nn.BatchNorm2d(488)\n",
    "\n",
    "        self.maxPool3=nn.MaxPool2d((2,2))\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fc1 = nn.Linear(32,3) \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        bs = len(x[:,0,0,0])\n",
    "        chans = len(x[0,:,0,0])\n",
    "        x = x.view(bs,1,chans,32,250)\n",
    "\n",
    "        x = self.conv1a(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.conv1a_bn(x)\n",
    "\n",
    "      \n",
    "        x = self.conv1b(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.conv1b_bn(x)\n",
    "        \n",
    "        x = self.maxPool1(x)\n",
    "\n",
    "        x = self.conv2a(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.conv2a_bn(x)\n",
    "\n",
    "\n",
    "        x = self.conv2b(x)\n",
    "        x = self.a4(x)\n",
    "        x = self.conv2b_bn(x)\n",
    "\n",
    "        x = self.maxPool2(x)\n",
    "        \"\"\"\n",
    "        x = self.conv3a(x)\n",
    "        x = self.a5(x)\n",
    "        x = self.conv3a_bn(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        x = self.conv3b(x)\n",
    "        x = self.a6(x)\n",
    "        x = self.conv3b_bn(x)\n",
    "\n",
    "        x = self.maxPool3(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.view(-1,32)\n",
    "\n",
    "        x = self.fc1(x) #a (1x1) output for each lead\n",
    "       # print(x.shape)\n",
    "\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#set to train w/ GPU if available else cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = W3DCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1r6uEfbx05D"
   },
   "source": [
    "# 2.) Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoqO4N0ZHWiO"
   },
   "source": [
    "### Optional: Run GBP on all mild moderate and severe examples\n",
    "gather all milds, moderate, and severe examples in respective file and run the following to create example lists of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJdi9LdvLzki"
   },
   "outputs": [],
   "source": [
    "ls1= []\n",
    "for fi in os.listdir('mild_examp'):\n",
    "    tmp = ('mild_examp/'+fi, 0)\n",
    "    ls1.append(tmp)\n",
    "\"\"\"\n",
    "ls1= []\n",
    "for fi in os.listdir('moderate_examp'):\n",
    "    tmp = ('moderate_examp/'+fi, 1)\n",
    "    ls1.append(tmp)\n",
    "ls1= []\n",
    "for fi in os.listdir('severe_examp'):\n",
    "    tmp = ('severe_examp/'+fi, 2)\n",
    "    ls1.append(tmp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXh0fjFSx05E"
   },
   "outputs": [],
   "source": [
    "def get_example_params(example_index, model, path_wts=None, examples=None):\n",
    "    \"\"\"\n",
    "        Gets used variables for almost all visualizations, like the image, model etc.\n",
    "    Args:\n",
    "        example_index (int): When examples = None, image id to use from examples\n",
    "        model: model: 1: WLCNN or 2: W3DCNN\n",
    "        path_wts: Path to trained model weights\n",
    "        examples: one of the example lists above (ls1)\n",
    "    returns:\n",
    "        original_image (numpy arr): Original image read from the file\n",
    "        prep_img (numpy_arr): Processed image\n",
    "        target_class (int): Target class for the image\n",
    "        file_name_to_export (string): File name to export the visualizations\n",
    "        pretrained_model(Pytorch model): Model to use for the operations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pick one of the examples\n",
    "    if examples == None:\n",
    "        example_list = (\n",
    "                        ('input_images/sub-032320_EC_mild_1_raw.fif', 0),\n",
    "                        ('input_images/sub-032464_EC_moderate_75_raw.fif', 1),\n",
    "                        ('input_images/sub-032455_EC_severe_101_raw.fif', 2)\n",
    "                    )\n",
    "    else:   \n",
    "        example_list = examples\n",
    "\n",
    "    img_path = example_list[example_index][0]\n",
    "    target_class = example_list[example_index][1]\n",
    "    file_name_to_export = img_path[img_path.rfind('/')+1:img_path.rfind('.')]\n",
    "    \n",
    "    # Process image\n",
    "    prep_img = preprocess_image(img_path)\n",
    "    original_image = prep_img\n",
    "    \n",
    "    # Define model\n",
    "    if model == 1:\n",
    "        pretrained_model = WLCNN()\n",
    "    elif model == 2:\n",
    "        pretrained_model = W3DCNN()\n",
    "    print(\"loading from: \"+path_wts)\n",
    "\n",
    "    checkpoint = torch.load(path_wts, map_location=torch.device('cpu'))\n",
    "    pretrained_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    #print(\"acc from prev:{:.4f}\".format(checkpoint['epoch_acc'])) \n",
    "\n",
    "    return (original_image,\n",
    "            prep_img,\n",
    "            target_class,\n",
    "            file_name_to_export,\n",
    "            pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "znS44vEox05H"
   },
   "outputs": [],
   "source": [
    "def convert_to_grayscale(im_as_arr):\n",
    "    \"\"\"\n",
    "        Converts 3d image to grayscale\n",
    "    Args:\n",
    "        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n",
    "    returns:\n",
    "        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n",
    "    \"\"\"\n",
    "    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n",
    "    im_max = np.percentile(grayscale_im, 99)\n",
    "    im_min = np.min(grayscale_im)\n",
    "    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n",
    "    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n",
    "    return grayscale_im\n",
    "\n",
    "def save_gradient_images(gradient, file_name):\n",
    "    \"\"\"\n",
    "        Exports the original gradient image\n",
    "    Args:\n",
    "        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n",
    "        file_name (str): File name to be exported\n",
    "    \"\"\"\n",
    "    #print(os.path.exists('./results'))\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "        print('dir made')\n",
    "    # Normalize\n",
    "    gradient = gradient - gradient.min()\n",
    "    gradient /= gradient.max()\n",
    "    # Save image\n",
    "    path_to_file = os.path.join('./results', file_name + '.jpg')\n",
    "    save_image(gradient, path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tHcmHdLQHvGv"
   },
   "outputs": [],
   "source": [
    "class HiddenPrints:\n",
    "    \"\"\"\n",
    "    Helper class to suppress mne print statements\n",
    "    \"\"\"\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wEHdEQwx05K"
   },
   "outputs": [],
   "source": [
    "def Wavelet(path:str):\n",
    "    \"\"\"\n",
    "    create data cubes using data from .csv file at path. of the form <\n",
    "    (Lead)x(frequencies)x(time) Power wavelete data types.\n",
    "\n",
    "    Args:\n",
    "        path to .csv file (should be passed by TesseractData and dataset)\n",
    "        ex: ~/SMNI_TRAINTEST_DATA/train/alcoholic/a_S3_377_069.csv\n",
    "\n",
    "    Returns: tess 3D numpy array containing time frequency transform data\n",
    "             of shape (Lead)x(frequency)x(time).\n",
    "    \"\"\"\n",
    "    channels = 61\n",
    "    scale = 32 #scale param for morle wavelet\n",
    "    length = 250\n",
    "\n",
    "    \n",
    "    raw = mne.io.read_raw_fif(path)\n",
    "    raw = raw.get_data(picks=raw.ch_names, start=0)\n",
    "\n",
    "    data = pd.DataFrame(data=raw)\n",
    "\n",
    "    y_voltage = np.empty([channels, length])\n",
    "    for i in range(channels):\n",
    "        y_voltage[i] = data.iloc[i]\n",
    "\n",
    "\n",
    "    # Generate a 3d array of channelXfreqXtime\n",
    "    waves_mag = np.empty([61, scale, 250 ], dtype=float) \n",
    "    # store the abs(coef) into waves_mag instead of computing waves_cmp\n",
    "    # then taking the absolute value of waves_cmp. I hope this speeds things up\n",
    "\n",
    "    # compute and store complex morlet transform for each lead\n",
    "    # THIS VERSION PUTS Lead First\n",
    "    for i in range(channels):\n",
    "        coef, freqs=pywt.cwt(y_voltage[i],np.arange(1,scale+1),'cmor0.4-1.0',sampling_period=1)\n",
    "        waves_mag[i,:,:] = copy.deepcopy(abs(coef))\n",
    "\n",
    "    return waves_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnWu_zJHx05N"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(path, resize_im=True):\n",
    "    \"\"\"\n",
    "        Processes image for CNNs\n",
    "    Args:\n",
    "        PIL_img (PIL_img): PIL Image or numpy array to process\n",
    "        resize_im (bool): Resize to 224 or not\n",
    "    returns:\n",
    "        im_as_var (torch variable): Variable that contains processed float tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    mean = [\n",
    "    6.057787360077358e-06,\n",
    "    6.087420497235737e-06,\n",
    "    7.3990380621281874e-06,\n",
    "    4.26021991865975e-06,\n",
    "    2.539638463888475e-06,\n",
    "    4.114347200376418e-06,\n",
    "    7.82875798194875e-06,\n",
    "    6.485739319775771e-06,\n",
    "    1.9192647558707106e-06,\n",
    "    2.473070259677167e-06,\n",
    "    7.191401187038389e-06,\n",
    "    9.746760885852078e-06,\n",
    "    6.5584541997872675e-06,\n",
    "    3.800515075142793e-06,\n",
    "    7.667242056921622e-06,\n",
    "    1.098459778463806e-05,\n",
    "    1.0834701802504087e-05,\n",
    "    8.28091010997476e-06,\n",
    "    9.337240549820418e-06,\n",
    "    1.2834776165816766e-05,\n",
    "    4.27243188727822e-06,\n",
    "    1.5791480155150454e-05,\n",
    "    1.4001914284608656e-05,\n",
    "    1.3751925491812879e-05,\n",
    "    1.5291240753861295e-05,\n",
    "    1.947148781941967e-05,\n",
    "    1.7938964069618997e-05,\n",
    "    1.987472439042673e-05,\n",
    "    1.845479014263127e-05,\n",
    "    1.968688715451437e-05,\n",
    "    1.8165726731432112e-05,\n",
    "    6.777176750486097e-06,\n",
    "    4.754154756755766e-06,\n",
    "    4.764475652547024e-06,\n",
    "    6.87751954700681e-06,\n",
    "    5.729166948061963e-06,\n",
    "    2.850098232526941e-06,\n",
    "    3.228867711013545e-06,\n",
    "    5.226466231830351e-06,\n",
    "    8.474320071937972e-06,\n",
    "    4.148349543128007e-06,\n",
    "    4.737125846372745e-06,\n",
    "    8.874316209515818e-06,\n",
    "    8.175465098051129e-06,\n",
    "    4.443676008117992e-06,\n",
    "    5.4173081548627475e-06,\n",
    "    9.241705597005122e-06,\n",
    "    1.237329241107373e-05,\n",
    "    9.616053051768428e-06,\n",
    "    8.303972253779376e-06,\n",
    "    1.1170897624137052e-05,\n",
    "    1.5039500493486429e-05,\n",
    "    1.4841113242660599e-05,\n",
    "    1.3065708563015742e-05,\n",
    "    1.3679599337093769e-05,\n",
    "    1.7859899457787005e-05,\n",
    "    1.9265918713678717e-05,\n",
    "    1.8492493007637822e-05,\n",
    "    1.7724493279678855e-05,\n",
    "    1.9086136839310716e-05,\n",
    "    2.2065214391782833e-05      \n",
    "    ]\n",
    "\n",
    "    dev = [\n",
    "    6.226787724404643e-06,\n",
    "    6.334461101252833e-06,\n",
    "    7.532137981857288e-06,\n",
    "    4.239707086526406e-06,\n",
    "    2.6669268673174235e-06,\n",
    "    4.085261767155552e-06,\n",
    "    8.121753271608523e-06,\n",
    "    6.6090573909851414e-06,\n",
    "    1.8582565106933258e-06,\n",
    "    2.516306726592908e-06,\n",
    "    7.5948400778550435e-06,\n",
    "    1.049799509243296e-05,\n",
    "    7.064685104230226e-06,\n",
    "    4.4963659700270465e-06,\n",
    "    8.31816463510664e-06,\n",
    "    1.2149702294967242e-05,\n",
    "    1.2092646101663928e-05,\n",
    "    9.530515704417179e-06,\n",
    "    1.1090785547727506e-05,\n",
    "    1.4668592146355854e-05,\n",
    "    4.48958615696688e-06,\n",
    "    1.9380885397891452e-05,\n",
    "    1.676695544359015e-05,\n",
    "    1.707348620669288e-05,\n",
    "    1.8257697630001285e-05,\n",
    "    2.3635050137116193e-05,\n",
    "    2.212982535067034e-05,\n",
    "    2.4296133802117022e-05,\n",
    "    2.227763445353628e-05,\n",
    "    2.3789918993075088e-05,\n",
    "    2.2138089933686428e-05,\n",
    "    6.8588371249472165e-06,\n",
    "    4.9794779785127445e-06,\n",
    "    4.944180658889942e-06,\n",
    "    7.057348321808729e-06,\n",
    "    5.829374656656345e-06,\n",
    "    2.916004499001558e-06,\n",
    "    3.5322337253732193e-06,\n",
    "    5.240591107098919e-06,\n",
    "    8.796968997040724e-06,\n",
    "    4.125534189419075e-06,\n",
    "    4.855986216498394e-06,\n",
    "    9.435575567982905e-06,\n",
    "    8.593776081275078e-06,\n",
    "    5.074781659449896e-06,\n",
    "    6.242808802312015e-06,\n",
    "    9.991918583732963e-06,\n",
    "    1.409254057377304e-05,\n",
    "    1.0894342352575892e-05,\n",
    "    9.875674795134744e-06,\n",
    "    1.2910368478758826e-05,\n",
    "    1.781295720914918e-05,\n",
    "    1.7783326314145297e-05,\n",
    "    1.5700134151438275e-05,\n",
    "    1.6390968593499346e-05,\n",
    "    2.1365723152930735e-05,\n",
    "    2.4059720280776322e-05,\n",
    "    2.262522612710923e-05,\n",
    "    2.1502597301469412e-05,\n",
    "    2.3205764264590672e-05,\n",
    "    2.7072069650931403e-05     \n",
    "    ]\n",
    "    \n",
    "    data_transforms = transforms.Compose([\n",
    "            transforms.Normalize(mean, dev)\n",
    "            ])\n",
    "    ten = torch.FloatTensor(Wavelet(path))\n",
    "    ten = data_transforms(ten)\n",
    "    ten.requires_grad=True\n",
    "    return ten.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOQo8FKvx05Q"
   },
   "source": [
    "# 3.) Guided Back Prop Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qLwJwyHPx05Q"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct 26 11:23:47 2017\n",
    "@author: Utku Ozbulak - github.com/utkuozbulak\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.nn import ReLU\n",
    "\n",
    "class GuidedBackprop():\n",
    "    \"\"\"\n",
    "       Produces gradients generated with guided back propagation from the given image\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.gradients = None\n",
    "        self.forward_relu_outputs = []\n",
    "        # Put model in evaluation mode\n",
    "        self.model.eval()\n",
    "        self.update_relus()\n",
    "        self.hook_layers()\n",
    "\n",
    "    def hook_layers(self):\n",
    "        def hook_function(module, grad_in, grad_out):\n",
    "            self.gradients = grad_in[0]\n",
    "            #print(grad_in[0])\n",
    "        # Register hook to the first layer\n",
    "        #print(list(self.model._modules.items())[0][1])\n",
    "        first_layer = list(self.model._modules.items())[0][1]\n",
    "        first_layer.register_backward_hook(hook_function)\n",
    "\n",
    "    def update_relus(self):\n",
    "        \"\"\"\n",
    "            Updates relu activation functions so that\n",
    "                1- stores output in forward pass\n",
    "                2- imputes zero for gradient values that are less than zero\n",
    "        \"\"\"\n",
    "        def relu_backward_hook_function(module, grad_in, grad_out):\n",
    "            \"\"\"\n",
    "            If there is a negative gradient, change it to zero\n",
    "            \"\"\"\n",
    "            # Get last forward output\n",
    "            corresponding_forward_output = self.forward_relu_outputs[-1]\n",
    "            corresponding_forward_output[corresponding_forward_output > 0] = 1\n",
    "            modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0)\n",
    "            del self.forward_relu_outputs[-1]  # Remove last forward output\n",
    "            #print(module, grad_in[0].shape)\n",
    "            return (modified_grad_out,)\n",
    "\n",
    "        def relu_forward_hook_function(module, ten_in, ten_out):\n",
    "            \"\"\"\n",
    "            Store results of forward pass\n",
    "            \"\"\"\n",
    "            self.forward_relu_outputs.append(ten_out)\n",
    "\n",
    "        # Loop through layers, hook up ReLUs\n",
    "        for pos, module in self.model._modules.items():\n",
    "            if isinstance(module, ReLU):\n",
    "                module.register_backward_hook(relu_backward_hook_function)\n",
    "                module.register_forward_hook(relu_forward_hook_function)\n",
    "\n",
    "    def generate_gradients(self, input_image, target_class):\n",
    "        #import pdb; pdb.set_trace()\n",
    "        # Forward pass\n",
    "        model_output = self.model(input_image)\n",
    "        # Zero gradients\n",
    "        self.model.zero_grad()\n",
    "        # Target for backprop\n",
    "        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n",
    "        one_hot_output[0][target_class] = 1\n",
    "        # Backward pass\n",
    "        model_output.backward(gradient=one_hot_output)\n",
    "        # Convert Pytorch variable to numpy array\n",
    "        gradients_as_arr = self.gradients.data.numpy()[0]\n",
    "        return gradients_as_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-LhbfQax05U"
   },
   "source": [
    "# 4.) Run Guided Back Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KaTG06uHx05V"
   },
   "outputs": [],
   "source": [
    "fs = 250 \n",
    "scale = 32\n",
    "file_name_to_export = \"guided_back_prop\"\n",
    "\n",
    "#path_wts = \"\"\n",
    "\n",
    "#create frequency scales\n",
    "frequencies = pywt.scale2frequency('cmor0.4-1.0', np.arange(1,scale+1)) / (1.0/fs)\n",
    "frequencies = frequencies.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4h71Tl4KIHv5"
   },
   "source": [
    "### Run GBP on one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "azHOMeNdx05c",
    "outputId": "2ffa9f9e-fc65-4e24-a9d6-b1da22fb8599",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## Get Params ############\n",
    "file_name_to_export = \"guided_back_prop\"\n",
    "target_example = 0  # alcoholic\n",
    "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) = get_example_params(example_index=0, model=1, path_wts=path_wts, examples=None)\n",
    "\n",
    "########## Guided backprop ############\n",
    "GBP = GuidedBackprop(pretrained_model)\n",
    "# Get gradients\n",
    "guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
    "\n",
    "print('Guided backprop completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n50oTrMM411D"
   },
   "source": [
    "### Run GBP on example list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DlmASmdhHUiH",
    "outputId": "2bcb04b8-52c1-4fca-e130-2f79541f8cb7"
   },
   "outputs": [],
   "source": [
    "x1 = np.zeros([61,32,250])\n",
    "x2 = np.zeros([1,61,32,250])\n",
    "examples = mild_examples\n",
    "path_wts = path_wts3\n",
    "print(\"Guided Back Prop...\")\n",
    "print(str(len(examples)) + \" examples...\")\n",
    "for i in range(len(examples)):\n",
    "    target_example = i\n",
    "    original_image, prep_img, target_class, file_name_to_export, pretrained_model = get_example_params(target_example, model = 2, path_wts = path_wts,examples=examples)\n",
    "\n",
    "    ########## Guided backprop ############\n",
    "    GBP = GuidedBackprop(pretrained_model)\n",
    "    # Get gradients\n",
    "    guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
    "    x1 += guided_grads[0]\n",
    "    x2 += prep_img.detach().numpy()\n",
    "x1 = x1/len(examples)\n",
    "x2 = x2/len(examples)\n",
    "print('Guided backprop completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKzYLWOoIVnU"
   },
   "source": [
    "### Create Mean activations:\n",
    "create mean activations one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 40
    },
    "colab_type": "code",
    "id": "OwnSp7QFLBI-",
    "outputId": "bd66dff9-526d-4754-bed1-7fa15408899e"
   },
   "outputs": [],
   "source": [
    "x1 = x1.mean(2)\n",
    "mi_act = np.sqrt(x1**2).T\n",
    "\n",
    "#uncomment for 3d \n",
    "x2 = x2[0]\n",
    "mi_img = x2[0].mean(0)\n",
    "print(mi_img.shape)\n",
    "\n",
    "\"\"\"\n",
    "x1 = x1.mean(2)\n",
    "mo_act = np.sqrt(x1**2).T\n",
    "\n",
    "#uncomment for 3d \n",
    "x2 = x2[0]\n",
    "mo_img = x2[0].mean(0)\n",
    "print(mo_img.shape)\n",
    "\n",
    "x1 = x1.mean(2)\n",
    "se_act = np.sqrt(x1**2).T\n",
    "\n",
    "#uncomment for 3d \n",
    "x2 = x2[0]\n",
    "se_img = x2[0].mean(0)\n",
    "print(se_img.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PqOW8nCx05f"
   },
   "source": [
    "### Plot Scalogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872
    },
    "colab_type": "code",
    "id": "p3Qu60QmPd61",
    "outputId": "e05df3e0-8b91-4ca6-d082-73138d7cba10"
   },
   "outputs": [],
   "source": [
    "#milx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#guided_grads = guided_grads.reshape(32,250)\n",
    "plt.figure(figsize=(15,15))\n",
    "#plt.title('GBP Saliency Map')\n",
    "#plt.ylabel('Frequency (Hz)')\n",
    "#plt.xlabel('Time (sec.)')\n",
    "plt.xticks([])\n",
    "plt.xticks([])\n",
    "\n",
    "#x = np.sqrt(guided_grads[0]**2)\n",
    "x = prep_img[0][60].detach().numpy()\n",
    "\n",
    "plt.yticks([])\n",
    "plt.imshow(x,extent=[0, 61, 0, 66],aspect='auto',\n",
    "            vmax=x.max(), vmin=x.min(), cmap = 'inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bhe5BaR2Ioev"
   },
   "source": [
    "### Plot Saliency Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "colab_type": "code",
    "id": "jkzSZ_VO0L5W",
    "outputId": "1ad05bdf-7aa3-46bf-f88a-e5f0910a27bb"
   },
   "outputs": [],
   "source": [
    "#milx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#guided_grads = guided_grads.reshape(32,250)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('GBP Saliency Map')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.xlabel('Time (sec.)')\n",
    "plt.xticks(np.arange(0,25.1,1),labels=np.round(np.arange(0,1.01,1./25),3),rotation=60, fontsize=18)\n",
    "plt.xticks(np.arange(1,62,1),labels=chan_names,rotation=60)\n",
    "\n",
    "#x = np.sqrt(guided_grads[0]**2)\n",
    "x = np.sqrt(guided_grads[0]**2)\n",
    "\n",
    "plt.yticks(np.arange(0,31.1,1),labels=np.flip(frequencies),rotation=45)\n",
    "plt.imshow(x,extent=[0, 61, 0, 31],aspect='auto',\n",
    "            vmax=x.max(), vmin=x.min(), cmap = 'plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i9cONb0_x05s"
   },
   "source": [
    "# 5.) Create Saliency and Two Channel Plot for Ea. example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYgEMT9sZgdJ"
   },
   "outputs": [],
   "source": [
    "plt_fr = np.array([frequencies[i] for i in range(0,len(frequencies),2)]) #Create new channels for label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "colab_type": "code",
    "id": "QhBrbSOGx05u",
    "outputId": "df3335b7-2e67-46c3-929f-c89fa8b276b9"
   },
   "outputs": [],
   "source": [
    "act_max = max(se_act.max(), mo_act.max(), mi_act.max())\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#create Big subplot\n",
    "fig = plt.figure(figsize=(24,10))  \n",
    "ax = fig.add_subplot(1,1,1)    # The big subplot\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "# Set common labels  \n",
    "ax.set_xlabel('Time (Sec.)',fontsize=24)\n",
    "ax.set_ylabel('Frequency (Hz)',fontsize=24)\n",
    "\n",
    "#Plot Saliency Map   \n",
    "ax1 = fig.add_subplot(1,3,1)\n",
    "ax1.set_title('Mild',fontsize=24)\n",
    "\n",
    "ax1.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax1.set_xticklabels(names, rotation = 70)\n",
    "ax1.set_yticks(np.arange(0,61,4), minor = False)\n",
    "ax1.set_yticklabels(np.flip(plt_fr))\n",
    "ax1.imshow(mi_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=mi_act.max(), vmin=0)\n",
    "\n",
    "#Plot Saliency Map   \n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax2.set_title('Moderate',fontsize=24)\n",
    "\n",
    "ax2.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax2.set_xticklabels(names, rotation = 70)\n",
    "ax2.set_yticks(np.arange(0,31.1,1), minor = False)\n",
    "ax2.set_yticklabels([])\n",
    "ax2.imshow(mo_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=mo_act.max(), vmin=0)\n",
    "\n",
    "#Plot Saliency Map   \n",
    "ax3 = fig.add_subplot(1,3,3)\n",
    "ax3.set_title('severe',fontsize=24)\n",
    "\n",
    "ax3.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax3.set_xticklabels(names, rotation = 70)\n",
    "ax3.set_yticks(np.arange(0,31.1,1), minor = False)\n",
    "ax3.set_yticklabels([])\n",
    "ax3.imshow(se_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=se_act.max(), vmin=0)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.06, hspace=0.02)\n",
    "\n",
    "\n",
    "# Turn off axis lines and ticks of the big subplot\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['bottom'].set_color('none')\n",
    "ax.spines['left'].set_color('none')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 662
    },
    "colab_type": "code",
    "id": "Z3JgyQBmNv9P",
    "outputId": "f6616fbf-0648-48cd-f51f-136ba2c05241"
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "#create Big subplot\n",
    "fig = plt.figure(figsize=(16,10), constrained_layout=False)  \n",
    "\n",
    "fig.add_gridspec(2, 4, wspace=0.0, hspace=0.001)\n",
    "#gs = gridspec.GridSpec(2, 4)\n",
    "#gs.update(wspace=0.5)\n",
    "ax1 = plt.subplot(gs[0, :2], )\n",
    "\n",
    "\n",
    "ax1.set_title('Mild',fontsize=24)\n",
    "ax1.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax1.set_xticklabels(names, rotation = 70)\n",
    "ax1.set_yticks(np.arange(0,61,4), minor = False)\n",
    "ax1.set_yticklabels(np.flip(plt_fr))\n",
    "ax1.imshow(mi_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=mi_act.max(), vmin=0)\n",
    "\n",
    "ax2 = plt.subplot(gs[0, 2:])\n",
    "ax2.set_title('Moderate',fontsize=24)\n",
    "ax2.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax2.set_xticklabels(names, rotation = 70)\n",
    "ax2.set_yticks(np.arange(0,31.1,1), minor = False)\n",
    "ax2.set_yticklabels([])\n",
    "ax2.imshow(mo_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=mo_act.max(), vmin=0)\n",
    "\n",
    "ax3 = plt.subplot(gs[1, 1:3])\n",
    "ax3.set_title('severe',fontsize=24)\n",
    "ax3.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax3.set_xticklabels(names, rotation = 70)\n",
    "ax3.set_yticks(np.arange(0,31.1,1), minor = False)\n",
    "ax3.set_yticklabels([])\n",
    "ax3.imshow(se_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=se_act.max(), vmin=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "E3TS1ARVERlh",
    "outputId": "b5a302a0-f192-4ef9-c8fc-f0d4322dff20"
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "#create Big subplot\n",
    "fig = plt.figure(figsize=(20,23), constrained_layout=False)  \n",
    "\n",
    "#fig.add_gridspec(2, 4, wspace=0.0, hspace=0.001)\n",
    "gs = gridspec.GridSpec(2, 4,wspace=0.15, hspace=0.15)\n",
    "#gs.update(wspace=0.5)\n",
    "ax1 = fig.add_subplot(gs[0, :2], )\n",
    "\n",
    "\n",
    "ax1.set_title('Mild',fontsize=40)\n",
    "ax1.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax1.set_xticklabels(names, rotation = 70,fontsize=30)\n",
    "ax1.set_yticks(np.arange(0,61,4), minor = False)\n",
    "ax1.set_yticklabels(np.flip(plt_fr),fontsize=30)\n",
    "ax1.imshow(mi_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=mi_act.max(), vmin=0)\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 2:])\n",
    "ax2.set_title('Moderate',fontsize=40)\n",
    "ax2.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax2.set_xticklabels(names, rotation = 70,fontsize=30)\n",
    "ax2.set_yticks(np.arange(0,61,4), minor = False)\n",
    "ax2.set_yticklabels([])\n",
    "#ax2.set_yticklabels(np.flip(plt_fr),fontsize=30)\n",
    "ax2.imshow(mo_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=mo_act.max(), vmin=0)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 1:3])\n",
    "ax3.set_title('Severe',fontsize=40)\n",
    "ax3.set_xticks(np.arange(1,62,4), minor = False)\n",
    "ax3.set_xticklabels(names, rotation = 70,fontsize=30)\n",
    "ax3.set_yticks(np.arange(0,61,4), minor = False)\n",
    "ax3.set_yticklabels(np.flip(plt_fr),fontsize=30)\n",
    "ax3.imshow(se_act,extent=[0, 61, 0, 61], aspect='equal',cmap = 'plasma',vmax=se_act.max(), vmin=0)\n",
    "\n",
    "#fig.subplots_adjust(wspace=-2, hspace=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrcZ8Vwfx05y"
   },
   "source": [
    "# 5.) Create Saliency and One Channel Plot for Ea. example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZqytkct4Q8k"
   },
   "outputs": [],
   "source": [
    "plt_fr = np.array([frequencies[i] for i in range(0,len(frequencies),6)]) #Create new channels for label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vOQ-yBc5x05y",
    "outputId": "6f765650-372a-4be9-ea60-67f1fc4ef902"
   },
   "outputs": [],
   "source": [
    "#create Big subplot\n",
    "fig = plt.figure(figsize=(17,17))   \n",
    "ax = fig.add_subplot(1,1,1)    # The big subplot\n",
    "\n",
    "# Set common labels  \n",
    "ax.set_xlabel('Time (Sec.)',fontsize=26)\n",
    "ax.set_ylabel('Frequency (Hz)',fontsize=26)\n",
    "\n",
    "#Compute GBP Map\n",
    "file_name_to_export = \"guided_back_prop\"\n",
    "target_example = 1 # \n",
    "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) = get_example_params(example_index=target_example,path_wts=path_wts1,model=0)\n",
    "GBP = GuidedBackprop(pretrained_model)\n",
    "guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
    "guided_grads = guided_grads.reshape(32,250)\n",
    "x = guided_grads\n",
    "\n",
    "#Compute GBP Map\n",
    "file_name_to_export = \"guided_back_prop\"\n",
    "target_example = 1  # \n",
    "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) = get_example_params(example_index=target_example,path_wts=path_wts2, model=1)\n",
    "GBP = GuidedBackprop(pretrained_model)\n",
    "guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
    "#guided_grads = guided_grads.reshape(32,250)\n",
    "#print(guided_grads.shape)\n",
    "x = guided_grads[16]\n",
    "\n",
    "#Plot Saliency Map     \n",
    "ax4 = fig.add_subplot(2,2,1)\n",
    "ax4.set_title('WLCNN',fontsize=24)\n",
    "ax4.set_xticks(np.arange(0,61.1,12.2), minor = False)\n",
    "ax4.set_xticklabels([])\n",
    "ax4.set_yticks(np.arange(0,61,4), minor = False)\n",
    "ax4.set_yticklabels(np.flip(plt_fr))\n",
    "ax4.imshow(np.sqrt(x**2), extent=[0, 61, 0, 61],aspect='equal',\n",
    "            vmax=np.sqrt(x**2).max(), vmin=0, cmap = 'plasma')\n",
    "\n",
    "#Plot CP5 Scalogram\n",
    "ax6 = fig.add_subplot(2,2,3)\n",
    "ax6.set_xticks(np.arange(0,25.1,5), minor = False)\n",
    "ax6.set_xticklabels(np.round(np.arange(0,1.1,0.2),2))\n",
    "ax6.set_xticks(np.arange(0,61.1,12.2), minor = False)\n",
    "ax6.set_xticklabels(np.round(np.arange(0,1.1,0.2),2))\n",
    "ax6.set_yticks(np.arange(0,61,4), minor = False)\n",
    "ax6.set_yticklabels(np.flip(plt_fr))\n",
    "for tick in ax6.xaxis.get_major_ticks():\n",
    "    tick.set_pad(8)\n",
    "ax6.imshow(prep_img.detach().numpy()[0][16], extent=[0, 61, 0, 61],aspect='equal',\n",
    "            vmax=0.75*abs(prep_img.detach()).max(), vmin=prep_img.detach().min(),cmap ='inferno')\n",
    "\n",
    "#Compute GBP Map\n",
    "file_name_to_export = \"guided_back_prop\"\n",
    "target_example = 1  #\n",
    "(original_image, prep_img, target_class, file_name_to_export, pretrained_model) = get_example_params(example_index=target_example,path_wts=path_wts3 ,model = 2)\n",
    "GBP = GuidedBackprop(pretrained_model)\n",
    "guided_grads = GBP.generate_gradients(prep_img, target_class)\n",
    "#guided_grads = guided_grads.reshape(32,250)\n",
    "x = guided_grads[0][16]\n",
    "\n",
    "#Plot Saliency Map    \n",
    "ax7 = fig.add_subplot(2,2,2)\n",
    "ax7.set_title('W3DCNN',fontsize=24)\n",
    "ax7.set_xticks(np.arange(0,25.1,5), minor = False)\n",
    "ax7.set_xticklabels([])\n",
    "ax7.set_yticks(np.arange(0,25.1,5), minor = False)\n",
    "ax7.set_yticklabels([])\n",
    "ax7.imshow(np.sqrt(x**2), extent=[-1, 25, -1, 25],aspect='equal',\n",
    "            vmax=np.sqrt(x**2).max(), vmin=0, cmap = 'plasma')\n",
    "\n",
    "#Plot CP5 Scalogram\n",
    "ax9 = fig.add_subplot(2,2,4)\n",
    "ax9.set_xticks(np.arange(0,25.1,5), minor = False)\n",
    "ax9.set_xticklabels(np.round(np.arange(0,1.1,0.2),2))\n",
    "ax9.set_yticks(np.arange(0,25.1,5), minor = False)\n",
    "ax9.set_yticklabels([])\n",
    "for tick in ax9.xaxis.get_major_ticks():\n",
    "    tick.set_pad(8)\n",
    "ax9.imshow(prep_img.detach().numpy()[0][16], extent=[-1, 25, -1, 25],aspect='equal',\n",
    "            vmax=0.75*abs(prep_img.detach()).max(), vmin=prep_img.detach().min(),cmap ='inferno')\n",
    "\n",
    "ax7.yaxis.set_label_position(\"right\")\n",
    "ax7.set_ylabel('Class Activatoin Map', fontsize=24)\n",
    "\n",
    "ax9.yaxis.set_label_position(\"right\")\n",
    "ax9.set_ylabel('Scalogram', fontsize=24)\n",
    "\n",
    "# Turn off axis lines and ticks of the big subplot\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['bottom'].set_color('none')\n",
    "ax.spines['left'].set_color('none')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.tick_params(labelcolor='w', top=False, bottom=False, left=False, right=False)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.01, hspace=0.05)\n",
    "\n",
    "\n",
    "fig.savefig('CP5_Scalo_Conv_Gbp.jpg')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Wave_Conv_Guided_Back_Prop.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
